from unsloth import FastLanguageModel
import torch
import os
import subprocess
from transformers import AutoModelForCausalLM




# Hugging Face æ¨¡å‹åç¨±
hf_model_name = "YShane11/llama3.2_flight"
local_model_path = "/root/LLM_unsloth/YShane11/llama3.2_flight"  # æœ¬åœ°æ¨¡å‹ä¸‹è¼‰ç›®éŒ„
hf_token = "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"  # è«‹ç¢ºä¿é€™æ˜¯ä½ çš„ Hugging Face Token

os.environ["HF_TOKEN"] = hf_token

# ä¸‹è¼‰å®Œæ•´ Hugging Face æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    hf_model_name, 
    cache_dir=local_model_path, 
    force_download=False,
    token=hf_token
)

# è¨­å®šé‡åŒ–æ–¹æ³•
quantization_methods = {
    "f16": "f16",
    "q4_k_m": "q8_0"
}

# ç¢ºä¿ gguf_models/ ç›®éŒ„å­˜åœ¨
output_dir = "/root/LLM_unsloth/gguf_models"
os.makedirs(output_dir, exist_ok=True)

# ç¢ºä¿ gguf_models/ ç›®éŒ„æœ‰å¯«å…¥æ¬Šé™
os.chmod(output_dir, 0o777)

# é‡‹æ”¾ CUDA è¨˜æ†¶é«”ï¼Œé¿å… OOM å•é¡Œ
torch.cuda.empty_cache()

# **æ‰‹å‹•è½‰æ› GGUF**
for method, outtype in quantization_methods.items():
    print(f"ğŸ”„ æ­£åœ¨é‡åŒ– {hf_model_name} ç‚º {method} ...")

    output_file = os.path.join(output_dir, f"{hf_model_name}_{method}.gguf")

    # **ç¢ºä¿æ¨¡å‹ç›®éŒ„å­˜åœ¨**
    if not os.path.exists(local_model_path):
        raise FileNotFoundError(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {local_model_path}")

    # **ä½¿ç”¨ convert_hf_to_gguf_update.py é€²è¡Œè½‰æ›**
    subprocess.run([
        "python3", "./llama.cpp/convert_hf_to_gguf_update.py",
        hf_token,
        local_model_path,  # æœ¬åœ°æ¨¡å‹ç›®éŒ„
        "--outfile", output_file,
        "--outtype", outtype,
    ], check=True)

    print(f"âœ… {method} é‡åŒ–å®Œæˆä¸¦å„²å­˜æ–¼ {output_file}ï¼")

# é‡‹æ”¾è¨˜æ†¶é«”
torch.cuda.empty_cache()
print("ğŸ‰ æ‰€æœ‰ GGUF é‡åŒ–å®Œæˆï¼")
