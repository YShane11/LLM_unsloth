from unsloth import FastLanguageModel
import torch
import os
import subprocess

# Hugging Face æ¨¡å‹åç¨±
hf_model_name = "YShane11/llama3.2_flight"
local_model_path = "/root/LLM_unsloth/YShane11/llama3.2_flight"  # ä¸‹è¼‰çš„æœ¬åœ°æ¨¡å‹è·¯å¾‘

# è¨­å®šé‡åŒ–æ–¹æ³•
quantization_methods = {
    "f16": "f16",
    "q4_k_m": "q8_0"
}

# ç¢ºä¿ `gguf_models/` ç›®éŒ„å­˜åœ¨
output_dir = "/root/LLM_unsloth/gguf_models"
os.makedirs(output_dir, exist_ok=True)

# ç¢ºä¿ `gguf_models/` ç›®éŒ„æœ‰å¯«å…¥æ¬Šé™
os.chmod(output_dir, 0o777)

# è¼‰å…¥æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=hf_model_name,
    max_seq_length=1024,  # è¨­å®šèˆ‡åŸå§‹æ¨¡å‹ç›¸åŒçš„ max_seq_length
    dtype=None,  # è®“ Unsloth è‡ªå‹•åˆ¤æ–·æœ€ä½³ç²¾åº¦
    load_in_4bit=True,  # ä½¿ç”¨ 4-bit é‡åŒ–ä»¥é™ä½è¨˜æ†¶é«”éœ€æ±‚
    token="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",
)

# é‡‹æ”¾ CUDA è¨˜æ†¶é«”ï¼Œé¿å… OOM å•é¡Œ
torch.cuda.empty_cache()

# **æ‰‹å‹•è½‰æ› GGUF**
for method, outtype in quantization_methods.items():
    print(f"ğŸ”„ æ­£åœ¨é‡åŒ– {hf_model_name} ç‚º {method} ...")

    output_file = os.path.join(output_dir, f"{hf_model_name}_{method}.gguf")

    # **ç¢ºä¿æ¨¡å‹ç›®éŒ„å­˜åœ¨**
    if not os.path.exists(local_model_path):
        raise FileNotFoundError(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {local_model_path}")

    # **ä½¿ç”¨ convert_hf_to_gguf.py é€²è¡Œè½‰æ›**
    subprocess.run([
        "python3", "./llama.cpp/convert_hf_to_gguf.py",
        local_model_path,  # æœ¬åœ°æ¨¡å‹ç›®éŒ„
        "--outfile", output_file,
        "--outtype", outtype
    ], check=True)

    print(f"âœ… {method} é‡åŒ–å®Œæˆä¸¦å„²å­˜æ–¼ {output_file}ï¼")

# é‡‹æ”¾è¨˜æ†¶é«”
torch.cuda.empty_cache()
print("ğŸ‰ æ‰€æœ‰ GGUF é‡åŒ–å®Œæˆï¼")
